# -*- coding: utf-8 -*-
"""Survey Paper

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAjd4qWTO3Qu9oASLT6t0QOgvIMxKMIB

## MAIN CODE START

Colab Notebook to run the model results for Biologically Inspired Oscillating Activation Functions
"""

#Pure (Same activation in both conv and dense on a chosen architecture)
# (Training and validation for all epochs, Test result for final epoch model)
# (Check loss and accuracy on lower epochs on graph)

#25 epochs cifar-10
#25 epochs cifar-100
#25 epochs imagenette

#Hybrid (relu,leakyrelu) (different activation in conv and dense on a chosen architecture)
# (Training and validation for all epochs, Test result for final epoch model)
# (Check loss and accuracy on lower epochs on graph)

#25 epochs cifar-10
#25 epochs cifar-100
#25 epochs imagenette
#tfds.image_classification.Imagenette#.load()

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension

!pip install keras-tuner
!pip install tensorflow-addons
# %tensorflow_version 2.x

# Clear any logs from previous runs
!rm -rf ./logs/

from google.colab import drive
drive.mount('/content/drive')

# Clear any logs from previous runs
!rm -rf /content/drive/My\ Drive/Colab\ Notebooks/SurveyPaper/logs/

from tensorflow.keras.layers import Activation
from sklearn.metrics import confusion_matrix
import tensorflow.keras.backend as K
import tensorflow_addons as tfa
import tensorflow as tf
import pandas as pd
import numpy as np
import math as m
import time
import gc
import os

#### Parameter for Gradient Calculation ######
gradient_cal = False
##############################################

# Define the Gradient Function
@tf.function
def get_gradient_func(model,loss_fn,tpu_in):
    with tf.GradientTape() as tape:
        logits = model(x_test, training=True)
        if tpu_in:
            loss = tf.reduce_sum(loss_fn(y_test, logits)) * (1. / batch_size)
        else:
            loss = loss_fn(y_test, logits)
    #print('loss\n',loss)
    grad = tape.gradient(loss, model.trainable_weights)
    #print('grad\n',grad)
    summed_squares = [K.sum(K.square(g)) for g in grad]
    norm = K.sqrt(sum(summed_squares))
    #inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights
    #func = K.function(inputs, [norm])
    #model.optimizer.apply_gradients(zip(grad, model.trainable_variables))
    return norm#.numpy()

class GradientCalcCallback(tf.keras.callbacks.Callback):
    def __init__(self, model_,loss_fn_,tpu_=None):
        self.model = model_
        self.loss_fn = loss_fn_
        self.tpu_in = tpu_
    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_time_start = time.time()
    def on_epoch_end(self, epoch, logs=None):
        #grads = get_gradient([x_test, y_test, np.ones(len(y_test))])
        grads = get_gradient_func(self.model,self.loss_fn,self.tpu_in)
        epoch_gradient.append(grads.numpy())
        times.append(time.time() - self.epoch_time_start)

class TimeCalcCallback(tf.keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_time_start = time.time()
    def on_epoch_end(self, epoch, logs=None):
        times.append(time.time() - self.epoch_time_start)

def act_sine(x):
    '''Sine'''
    return tf.sin(x)

def GCU(x):
    '''Growing Cosine Unit'''
    return x * tf.cos(x)

def cos_2(x):
    '''Cos_2'''
    return tf.square(x) * tf.cos(x)

def act_sign_sin(x):
    '''Act_Sign_Sin'''
    return tf.sign(x)*tf.square(tf.sin(x))

#def swish(x):
#   '''Swish'''
#  return x * tf.sigmoid(x)

def new_act_1(x):
    '''New_Act_1'''
    return tf.math.pow(x + 1, 3)

pi = tf.constant(m.pi)

def act_signum(x):
    '''Signum'''
    return tf.sign(x)

def act_identity(x):
    '''Identity'''
    return tf.identity(x)

#def act_step(x):
#    '''Step'''
#    #tf.step(x)
#    if tf.greater_equal(x,0):
#      return 1
#    return 0

def act_SiLU(x):
    '''SiLU'''
    return (x/(1+tf.math.exp(-x)))

def act_LiSHT(x):
    '''LiSHT'''
    return x*tf.tanh(x)

def act_ReSech(x):
    '''ReSech'''
    return x/tf.tanh(x)

def act_bipolar(x):
    '''Bipolar'''
    return ((1-tf.math.exp(-x)) / (1+tf.math.exp(-x)))

def act_absolute(x):
    '''Absolute'''
    return abs(x)

def act_elliott(x):
    '''Elliott'''
    return (1/(1+abs(x)))

def act_quadratic(x):
    '''Quadratic'''
    return (tf.math.pow(x,2) + x)

def act_Mcubic(x):
    '''Monotonic Cubic'''
    return (tf.math.pow(x,3) + x)

def act_NMcubic(x):
    '''Non Monotonic Cubic'''
    return (x - tf.math.pow(x,3))

def act_DSU(x):
    '''Decaying Sine Unit'''
    return (pi/2)*(tf.experimental.numpy.sinc(x-pi) - tf.experimental.numpy.sinc(x+pi)) #((tf.sin(x - pi)/(x - pi)) - (tf.sin(x + pi)/(x + pi)))

#actual neuronal voltages, action potential
def act_shiftedSinc(x):
    '''Shifted Sinc'''
    return pi*tf.experimental.numpy.sinc(x-pi)

#################### TO BE DONE #########################
## FILL IN NEW ACTIVATION FUNCTIONS HERE (IF ANY)

#'ThresholdedReLU',new_act_1,softmax,exponential,act_elliott,act_ReSech

activation_inputs = ['sigmoid','softplus','softsign',
                     'relu','PReLU','tanh','elu',
                     'LeakyReLU',act_shiftedSinc,
                     'selu',act_absolute,GCU,act_NMcubic,
                     act_LiSHT,act_Mcubic,act_quadratic,
                     act_sine,tf.keras.activations.swish,act_signum,
                     act_identity,'gelu',act_DSU,act_SiLU,
                     cos_2,tfa.activations.mish,act_sign_sin,act_bipolar]

#if gradient_cal:
#    activation_inputs = ['gelu',act_DSU,act_SiLU,
#                         cos_2,tfa.activations.mish,act_sign_sin,act_bipolar]

#    activation_inputs = ['sigmoid','softplus','softsign',
#                         'relu','PReLU','tanh','elu',
#                         'LeakyReLU']

#    activation_inputs = [act_shiftedSinc,
#                         'selu',act_absolute,GCU,act_NMcubic,
#                         act_LiSHT,act_Mcubic,act_quadratic,
#                         act_sine,tf.keras.activations.swish,act_signum,act_identity]

for q in range(0,1):
  for k in activation_inputs:
      if type(k) == str:
          continue
      print(k(float(q)))
      print(Activation(k))

"""VGG CODE START"""

!rm -r ~/tensorflow_datasets/imagenette
!rm -r ~/tensorflow_datasets/downloads

import tensorflow_datasets as tfds
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, LeakyReLU
from tensorflow.keras.layers import Conv2D, MaxPool2D#MaxPooling2D
#!tfds build --register_checksums

try:
    #data, info = tfds.load('imagenette', with_info=True, as_supervised=True)
    data, info = tfds.load("imagenette/160px-v2", with_info=True, as_supervised=True) #imagenette
except:
    #data, info = tfds.load('imagenette', with_info=True, as_supervised=True)
    data, info = tfds.load("imagenette/160px-v2", with_info=True, as_supervised=True) #imagenette

train_data, test_data = data['train'], data['validation']

train_dataset = train_data.map(
    lambda image, label: (tf.image.resize(image, (160, 160)), label))

test_dataset = test_data.map(
    lambda image, label: (tf.image.resize(image, (160, 160)), label)
)

del data
del train_data
del test_data
gc.collect()

data_list = []
data_list_test = []

num_classes = info.features['label'].num_classes
print(f'Total number of classes in dataset is {num_classes}')

get_label_name = info.features['label'].int2str
text_labels = [get_label_name(i) for i in range(num_classes)]
for idx,i in enumerate(text_labels):
    print(f'The Label {idx} name is `{i}`')

x_train = list(map(lambda x: x[0], train_dataset))
x_test = list(map(lambda x: x[0], test_dataset))
x_train = np.array(x_train)
x_test = np.array(x_test)

y_train = list(map(lambda x: x[1], train_dataset))
y_test = list(map(lambda x: x[1], test_dataset))
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test =  tf.keras.utils.to_categorical(y_test, num_classes)
y_train = np.array(y_train)
y_test = np.array(y_test)

if gradient_cal:
    x_train= tf.image.rgb_to_grayscale(x_train).numpy()
    x_test= tf.image.rgb_to_grayscale(x_test).numpy()

    x_train = np.append(x_train,x_test[:3500],0)
    x_test = x_test[3500:]
    print(x_train.shape,x_test.shape)

    y_train = np.append(y_train,y_test[:3500],0)
    y_test = y_test[3500:]
    print(y_train.shape,y_test.shape)

x_train, x_test = x_train/255.0, x_test/255.0

del train_dataset
del test_dataset
gc.collect()

train_len = info.splits['train'].num_examples
test_len = info.splits['validation'].num_examples
print(f'Train size {train_len} and Test size {test_len}')
y_train.shape,y_test.shape

#checkpoint = ModelCheckpoint("vgg16_1.h5", monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)
#early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=0, mode='auto')

def vgg(activation_fn,dense_activation_fn):
    model = Sequential()
    model.add(Conv2D(input_shape=x_train.shape[1:],filters=64,kernel_size=(3,3),padding="same", activation=activation_fn)) #(160,160,3)
    model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation=activation_fn))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation=activation_fn))
    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
    model.add(Flatten())
    model.add(Dense(units=4096,activation=dense_activation_fn))
    model.add(Dense(units=4096,activation=dense_activation_fn))
    model.add(Dense(units=10, activation="softmax"))
    return model

#def new model function#
#training procedure

def training_vgg(activation_fn,dense_activation_fn):
    K.clear_session()
    model = vgg(activation_fn,dense_activation_fn)
    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)#, decay=1e-6)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

    if gradient_cal:
        loss_fn = tf.keras.losses.CategoricalCrossentropy()
        history = model.fit(x_train,
                            y_train,
                            #generator=traindata,
                            #steps_per_epoch=100,
                            #validation_data= testdata,
                            batch_size=16,
                            validation_split = 0.2,
                            shuffle=True,
                            #validation_steps=10,
                            epochs=25,
                            callbacks=[GradientCalcCallback(model,loss_fn)]) #checkpoint, #early,
    else:
        history = model.fit(x_train,
                            y_train,
                            #generator=traindata,
                            #steps_per_epoch=100,
                            #validation_data= testdata,
                            batch_size=32,
                            validation_split = 0.2,
                            shuffle=True,
                            #validation_steps=10,
                            epochs=25,
                            callbacks=[TimeCalcCallback()]) #checkpoint, #early,

    history = history.history
    test_loss, test_accuracy = model.evaluate(x_test,y_test)#,batch_size=batch_size)

    p_test = model.predict(x_test,verbose=0).argmax(axis=1)
    y_test_ = np.argmax(y_test, axis=1)
    cm = confusion_matrix(y_test_, p_test)

    FP = cm.sum(axis=0) - np.diag(cm)
    FN = cm.sum(axis=1) - np.diag(cm)
    TP = np.diag(cm)
    TN = cm.sum() - (FP + FN + TP)

    # Sensitivity, hit rate, recall, or true positive rate
    TPR = TP/(TP+FN)
    # Specificity or true negative rate
    TNR = TN/(TN+FP)
    # Precision or positive predictive value
    PPV = TP/(TP+FP)
    # Negative predictive value
    NPV = TN/(TN+FN)
    # Fall out or false positive rate
    FPR = FP/(FP+TN)
    # False negative rate
    FNR = FN/(TP+FN)
    # False discovery rate
    FDR = FP/(TP+FP)

    act_func_str = activation_fn

    if act_func_str == tfa.activations.mish:
        act_func_str = 'Mish'

    if act_func_str == tf.keras.activations.swish:
        act_func_str = 'Swish'

    if type(act_func_str) != str:
        act_func_str = act_func_str.__doc__

    history.update({'activation_function':act_func_str,'epochs':list(range(1,len(history['accuracy'])+1))})
    data_list_test.append([act_func_str,test_loss,test_accuracy,
                           TP,TN,FP,FN,TPR,TNR,PPV,NPV,FPR,FNR,FDR,cm])
    del model

    return history


for activation_input in zip(activation_inputs,activation_inputs): #['relu']*len(activation_inputs)
    print(activation_input)
    #(relu,relu)
    #(tanh,relu)
    #(quadratic,relu)
    times = []
    history_ = training_vgg(activation_input[0],activation_input[1]) #(conv_activation, dense_activation)
    history_.update({'time_taken_on_epoch':times})
    data_list.append(pd.DataFrame(history_))
    gc.collect()

data = pd.concat(data_list)
data_test = pd.DataFrame(data_list_test)
data_test = data_test.rename(columns={0:'activation_function',1:'test_loss',2:'test_accuracy',
                                      3:'TP',4:'TN',5:'FP',6:'FN',7:'TPR',8:'TNR',9:'PPV',10:'NPV',11:'FPR',12:'FNR',13:'FDR',14:'cm'})
data.to_csv('/content/drive/My Drive/Colab Notebooks/SurveyPaper/vggGradSet1_resuts_1.csv',index=False) #
data_test.to_pickle('/content/drive/My Drive/Colab Notebooks/SurveyPaper/vggGradSet1_resuts_cm_1.pkl') #
data

"""VGG CODE END

CIFAR 10 & CIFAR 100 CODE START
"""

#from tensorflow.python.ops.numpy_ops import np_config
#np_config.enable_numpy_behavior()
#tf.compat.v1.enable_v2_behavior()
#from tensorboard.plugins.hparams import api as hp
#from kerastuner.engine.hyperparameters import HyperParameters as HP
#tf.keras.backend.clear_session()  # For easy reset of notebook state.
#tf.compat.v1.disable_eager_execution()
#tf.compat.v1.disable_v2_behavior()
#%load_ext tensorboard
#fashion_mnist = tf.keras.datasets.fashion_mnist
#csv_logger = tf.keras.callbacks.CSVLogger('/content/drive/My Drive/Colab Notebooks/SurveyPaper/training.log')

#import tensorflow.keras
#from tensorflow.keras.preprocessing.image import ImageDataGenerator
#from tensorboard.plugins.hparams import api as hp
#from keras.callbacks import ModelCheckpoint, EarlyStopping

from tensorflow.keras.datasets import cifar100 #cifar10 #
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, LeakyReLU
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from sklearn.model_selection import StratifiedShuffleSplit


#gpu_info = !nvidia-smi
#gpu_info = '\n'.join(gpu_info)
#if gpu_info.find('failed') >= 0:
#  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
#  print('and then re-execute this cell.')
#else:
#  print(gpu_info)
#################################################################################################################################################################################
num_classes = 100 #10 #
epochs = 25
batch_size = None # To be defined below as per conditions below #

data_list = []
data_list_test = []
(x_train, y_train),(x_test, y_test) = cifar100.load_data() #cifar10
x_train, x_test = x_train.astype('float32'), x_test.astype('float32')

if gradient_cal:
    x_train= tf.image.rgb_to_grayscale(x_train)
    x_test= tf.image.rgb_to_grayscale(x_test)

x_train, x_test = x_train / 255.0, x_test / 255.0

if gradient_cal:

    x_train = np.append(x_train,x_test[:5000],0)
    x_test = x_test[5000:]
    print(x_train.shape,x_test.shape)

    y_train = np.append(y_train,y_test[:5000],0)
    y_test = y_test[5000:]
    print(y_train.shape,y_test.shape)

initial_model = True
__gpu = True
__tpu = None

if __tpu:
    #RGB to GrayScale for faster training
    x_train=np.dot(x_train[...,:3], [0.299, 0.587, 0.114])
    x_test=np.dot(x_test[...,:3], [0.299, 0.587, 0.114])
    # add empty color dimension
    x_train = np.expand_dims(x_train, -1)
    x_test = np.expand_dims(x_test, -1)

if initial_model:
    batch_size = 32
    y_train = tf.keras.utils.to_categorical(y_train, num_classes)
    y_test = tf.keras.utils.to_categorical(y_test, num_classes)
else:
    batch_size = 128
    y_train, y_test = y_train.flatten(), y_test.flatten()
    #Split the dataset into train and valid
    s = StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=1/6)
    train_index, valid_index = next(s.split(x_train, y_train))
    x_valid, y_valid = x_train[valid_index], y_train[valid_index]
    x_train, y_train = x_train[train_index], y_train[train_index]
    print(x_train.shape, x_valid.shape, x_test.shape)

print(x_train.shape,x_test.shape)

print(batch_size,epochs)
#def get_gradient_norm_func(model):
#    grads = K.gradients(model.total_loss, model.trainable_weights)
#    summed_squares = [K.sum(K.square(g)) for g in grads]
#    norm = K.sqrt(sum(summed_squares))
#    inputs = model._feed_inputs + model._feed_targets + model._feed_sample_weights
#    func = K.function(inputs, [norm])
#    return func

#get_gradient = get_gradient_norm_func(cifar10_model)


        #grads = get_gradient([x_test, y_test, np.ones(len(y_test))])
        #grads = get_gradient_func(self.model,self.loss_fn)
        #epoch_gradient.append(grads.numpy())

#datagen = tf.keras.preprocessing.image.ImageDataGenerator(
#    rotation_range=15,
#    width_shift_range=0.1,
#    height_shift_range=0.1,
#    horizontal_flip=True,
#    )
#datagen.fit(x_train)

#early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, verbose=1)

# kernel_initializer=tf.contrib.layers.xavier_initializer()

def model_initial(activation, dense_activation):
    print('initial_model')
    model = Sequential()
    model.add(Conv2D(32, (3, 3), padding='same',
                     input_shape=x_train.shape[1:]))
    model.add(Activation(activation))
    model.add(Conv2D(32, (3, 3)))
    model.add(Activation(activation))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(Activation(activation))
    model.add(Conv2D(64, (3, 3)))
    model.add(Activation(activation))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation(dense_activation))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))

    return model

def model_1(activation_fn,dense_activation_fn=None,is_tpu=None):
    cifar10_model=tf.keras.models.Sequential()
    cifar10_model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding="same", activation= activation_fn, input_shape=[32,32,3]))
    cifar10_model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding="same", activation= activation_fn))
    cifar10_model.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2,padding='valid'))
    cifar10_model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding="same", activation= activation_fn))
    cifar10_model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding="same", activation= activation_fn))
    cifar10_model.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2,padding='valid'))
    cifar10_model.add(tf.keras.layers.Flatten())
    cifar10_model.add(tf.keras.layers.Dropout(0.5,noise_shape=None,seed=None))
    cifar10_model.add(tf.keras.layers.Dense(units=128,activation= dense_activation_fn))
    cifar10_model.add(tf.keras.layers.Dense(units=num_classes,activation='softmax'))
    #if is_tpu:
    #    pass
    #else:
    #    cifar10_model.add(tf.keras.layers.Dense(units=10,activation='softmax'))
    return cifar10_model

def model_2(activation_fn,dense_activation_fn=None,is_tpu=None):
    model = tf.keras.models.Sequential()
    #model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))
    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation=activation_fn))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))
    model.add(tf.keras.layers.Dropout(0.25))

    #model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))
    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation=activation_fn))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Dropout(0.25))

    #model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))
    model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation=activation_fn))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))
    model.add(tf.keras.layers.Dropout(0.25))

    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(256))
    model.add(tf.keras.layers.Activation(dense_activation_fn))
    model.add(tf.keras.layers.Dropout(0.5))
    model.add(tf.keras.layers.Dense(10))
    model.add(tf.keras.layers.Activation('softmax'))
    #if is_tpu:
    #    pass
    #else:
    #    model.add(tf.keras.layers.Activation('softmax'))
    return model

def train_test_model(activation_fn,dense_activation_fn=None,x_train=None,y_train=None,x_valid=None,y_valid=None,x_test=None,y_test=None,__gpu=None,__tpu=None,initial_model=None):
    K.clear_session()
    '''
    --> TRY PURE RUN (ONE ACTIVATION FUNCTION IN BOTH CONV AND DENSE) FIRST THEN TRY COMBINATION OF ACTIVATION FUNCTIONS ON CONV AND DENSE LAYERS RESPECTIVELY
    --> DECREASE CONVOLUTION LAYERS AND CHECK OUTPUT
    --> CHECK ON 64 UNITS AS WELL AFTER INITIAL RUNS
    --> CHECK AT MSE AND 1-1 BIPOLAR IN PLACE OF SOFTMAX
    --> FETCH TRAIN ACCURACY/LOSS, VALIDATION ACCURACY/LOSS, VALIDATION GRADIENT (GRADIENT CALCULATED WITH VALIDATION SET), TRAINING TIME PER EPOCH
    --> CHECK IF LAYER WISE GRADIENTS CAN BE FETCHED OR NOT
    '''
    cm = None
    history = None
    p_test = None
    test_loss = None
    test_accuracy = None
    y_test_ = None

    if __gpu:
        cifar10_model = None
        if initial_model:
            cifar10_model = model_initial(activation_fn,dense_activation_fn)
            # initiate RMSprop optimizer
            opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)
            # Let's train the model using RMSprop
            cifar10_model.compile(loss='categorical_crossentropy',
                                  optimizer=opt,
                                  metrics=['accuracy'])
        else:
            cifar10_model = model_2(activation_fn,dense_activation_fn)
            cifar10_model.compile(loss='sparse_categorical_crossentropy', optimizer="Adam", metrics=["sparse_categorical_accuracy"])
            #cifar10_model.compile(loss="sparse_categorical_crossentropy", optimizer="Adam", metrics=["sparse_categorical_accuracy"])


        if gradient_cal:
            if initial_model:
                loss_fn = tf.keras.losses.CategoricalCrossentropy()
                history = cifar10_model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_split = 0.2,shuffle=True,callbacks=[GradientCalcCallback(cifar10_model,loss_fn)])
            else:
                loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
                history = cifar10_model.fit(x_train,y_train,epochs=epochs,validation_data=(x_valid,y_valid), batch_size=batch_size, callbacks=[GradientCalcCallback(cifar10_model,loss_fn)])
        else:
            if initial_model:
                #print(y_train.shape)
                history = cifar10_model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_split = 0.2,shuffle=True,callbacks=[TimeCalcCallback()])
            else:
                history = cifar10_model.fit(x_train,y_train,epochs=epochs,validation_data=(x_valid,y_valid),shuffle=True, batch_size=batch_size, callbacks=[TimeCalcCallback()])

        test_loss, test_accuracy = cifar10_model.evaluate(x_test, y_test,batch_size=batch_size)


        if initial_model:
            p_test = cifar10_model.predict(x_test,batch_size=batch_size,verbose=0)#.argmax(axis=1) #.argmax(axis=1) #predict_classes
            p_test = np.argmax(p_test, axis=1)
            y_test_ = np.argmax(y_test, axis=1)
            #print(p_test.shape,y_test.shape)
        else:
            p_test = cifar10_model.predict(x_test,batch_size=batch_size,verbose=0).argmax(axis=1)

        cm = confusion_matrix(y_test_, p_test)

    elif __tpu:
        #tpu_model = tf.contrib.tpu.keras_to_tpu_model(
        #    model,
        #    strategy=tf.contrib.tpu.TPUDistributionStrategy(
        #        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
        #    ))
        #resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
        #tf.config.experimental_connect_to_host(resolver.master())
        #tf.tpu.experimental.initialize_tpu_system(resolver)
        #strategy = tf.distribute.experimental.TPUStrategy(resolver)
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
        tf.config.experimental_connect_to_cluster(tpu)
        tf.tpu.experimental.initialize_tpu_system(tpu)
        strategy = tf.distribute.experimental.TPUStrategy(tpu)
        print("REPLICAS: ", strategy.num_replicas_in_sync)

        model = None
        loss_fn = None

        with strategy.scope():
            if initial_model:
                model = model_initial(activation_fn,dense_activation_fn)
                # initiate RMSprop optimizer
                opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)
                # Let's train the model using RMSprop
                model.compile(loss='categorical_crossentropy',
                                      optimizer=opt,
                                      metrics=['accuracy'])
            else:
                model = model_2(activation_fn,dense_activation_fn)
                model.compile(
                    optimizer='adam',#tf.train.AdamOptimizer(learning_rate= 0.001 ),
                    loss='sparse_categorical_crossentropy',#tf.keras.losses.sparse_categorical_crossentropy,
                    metrics=['sparse_categorical_accuracy'])

            if gradient_cal:
                if initial_model:
                    loss_fn = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)#from_logits=True, #reduction=tf.keras.losses.Reduction.SUM)
                    cb = GradientCalcCallback(model,loss_fn,__tpu)
                else:
                    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)#from_logits=True, #reduction=tf.keras.losses.Reduction.SUM)
                    cb = GradientCalcCallback(model,loss_fn,__tpu)

        if gradient_cal:
            if initial_model:
                history = model.fit(x_train, y_train,
                                            batch_size=batch_size,
                                            steps_per_epoch=int(np.ceil(x_train.shape[0] / batch_size)),
                                            epochs=epochs,
                                            validation_split = 0.2,
                                            shuffle=True,
                                            callbacks=[cb])
            else:
                #datagen.flow(x_train, y_train, batch_size=batch_size)
                history=model.fit(x_train,y_train,
                    epochs=epochs,
                    steps_per_epoch=int(np.ceil(x_train.shape[0] / batch_size)),
                    validation_data=(x_valid, y_valid),shuffle=True,callbacks=[cb]) #early_stop,
        else:
            #datagen.flow(x_train, y_train, batch_size=batch_size)
            if initial_model:
                history = model.fit(x_train, y_train,
                                            batch_size=batch_size,
                                            steps_per_epoch=int(np.ceil(x_train.shape[0] / batch_size)),
                                            epochs=epochs,
                                            validation_split = 0.2,
                                            shuffle=True,
                                            callbacks=[TimeCalcCallback()])
            else:
                history=model.fit(x_train,y_train,
                                  epochs=epochs,
                                  steps_per_epoch=int(np.ceil(x_train.shape[0] / batch_size)),
                                  validation_data=(x_valid, y_valid),
                                  shuffle=True,
                                  callbacks=[TimeCalcCallback()]) #early_stop,

        test_loss, test_accuracy = model.evaluate(x_test, y_test,batch_size=batch_size)
        model.save_weights('cifar_10.h5', overwrite=True)
        #cpu_model = model.sync_to_cpu()
        cpu_model = None
        if initial_model:
            cpu_model = model_initial(activation_fn,dense_activation_fn)
        else:
            cpu_model = model_2(activation_fn,dense_activation_fn)

        cpu_model.load_weights('cifar_10.h5')

        if initial_model:
            p_test = cifar10_model.predict(x_test,batch_size=batch_size,verbose=0)#.argmax(axis=1) #.argmax(axis=1) #predict_classes
            p_test = np.argmax(p_test, axis=1)
            y_test_ = np.argmax(y_test, axis=1)
            #print(p_test.shape,y_test.shape)
        else:
            p_test = cifar10_model.predict(x_test,batch_size=batch_size,verbose=0).argmax(axis=1)

        cm = confusion_matrix(y_test_, p_test)


    ####### METRICS ###########
    history = history.history
    FP = cm.sum(axis=0) - np.diag(cm)
    FN = cm.sum(axis=1) - np.diag(cm)
    TP = np.diag(cm)
    TN = cm.sum() - (FP + FN + TP)

    # Sensitivity, hit rate, recall, or true positive rate
    TPR = TP/(TP+FN)
    # Specificity or true negative rate
    TNR = TN/(TN+FP)
    # Precision or positive predictive value
    PPV = TP/(TP+FP)
    # Negative predictive value
    NPV = TN/(TN+FN)
    # Fall out or false positive rate
    FPR = FP/(FP+TN)
    # False negative rate
    FNR = FN/(TP+FN)
    # False discovery rate
    FDR = FP/(TP+FP)

    act_func_str = activation_fn

    if act_func_str == tfa.activations.mish:
        act_func_str = 'Mish'

    if act_func_str == tf.keras.activations.swish:
        act_func_str = 'Swish'

    if type(act_func_str) != str:
        act_func_str = act_func_str.__doc__

    data_list_test.append([act_func_str,test_loss,test_accuracy,
                           TP,TN,FP,FN,TPR,TNR,PPV,NPV,FPR,FNR,FDR,cm])
    #if __gpu:
    #    history.update({'activation_function':act_func_str,'epochs':list(range(1,26))})
    #elif __tpu:
    #    history.update({'activation_function':act_func_str,'epochs':list(range(1,31))})
    history.update({'activation_function':act_func_str,'epochs':list(range(1,len(history['accuracy'])+1))})
    #history.update({'activation_function':act_func_str,'epochs':list(range(1,16)),'time_taken_on_epoch':times,'val_gradient':list(map(lambda k: k[0],epoch_gradient))})

    if __gpu:
        del cifar10_model
    elif __tpu:
        del model

    if gradient_cal:
        del loss_fn

    return history

epoch_gradient = None
times = None
#activation_combinations = []
#for x in ['LeakyReLU','PReLU','selu']:
#    for y in activation_inputs:
#        activation_combinations.append((y,x))
#len(activation_combinations)

for activation_input in zip(activation_inputs,activation_inputs): #zip(activation_inputs,activation_inputs)) activation_combinations
    print(activation_input)
    if gradient_cal:
        epoch_gradient = []
    times = []
    # __gpu=True #Can use only TPU or GPU at a time, for CPU run use GPU mode only #
    if initial_model:
        history_ = train_test_model(activation_input[0],activation_input[1],
                                    x_train=x_train,y_train=y_train,
                                    x_test=x_test,y_test=y_test,
                                    __gpu=__gpu,__tpu=__tpu,
                                    initial_model=initial_model)
    else:
        history_ = train_test_model(activation_input[0],activation_input[1],
                                    x_train=x_train,y_train=y_train,
                                    x_test=x_test,y_test=y_test,
                                    x_valid=x_valid,y_valid=y_valid,
                                    __gpu=__gpu,__tpu=__tpu,) ##initial_model=initial_model)
    if gradient_cal:
        history_.update({'time_taken_on_epoch':times,'test_gradient':epoch_gradient})
    else:
        history_.update({'time_taken_on_epoch':times})
    data_list.append(pd.DataFrame(history_))
    gc.collect()

data = pd.concat(data_list)
data_test = pd.DataFrame(data_list_test)
data_test = data_test.rename(columns={0:'activation_function',1:'test_loss',2:'test_accuracy',
                                      3:'TP',4:'TN',5:'FP',6:'FN',7:'TPR',8:'TNR',9:'PPV',10:'NPV',11:'FPR',12:'FNR',13:'FDR',14:'cm'})
data.to_csv('/content/drive/My Drive/Colab Notebooks/SurveyPaper/cifar_100GradSet2_resuts_1.csv',index=False) #
data_test.to_pickle('/content/drive/My Drive/Colab Notebooks/SurveyPaper/cifar_100GradSet2_resuts_cm_1.pkl') #
data

#train_datagen = ImageDataGenerator(
#      rescale=1./255,
#      rotation_range=40,
#      height_shift_range=0.2)

#valid_datagen = ImageDataGenerator(
#      rescale=1./255)


#train_ds = tf.keras.preprocessing.image.NumpyArrayIterator(
#    x=np.array(x_train), y=np.array(y_train), image_data_generator=train_datagen,batch_size=16
#)

#valid_ds = tf.keras.preprocessing.image.NumpyArrayIterator(
#    x=np.array(X_valid), y=np.array(y_valid), image_data_generator=valid_datagen,batch_size=32
#)

# train_datagen.fit(X_train)

#history = model.fit(
#  train_ds,
#  validation_data=valid_ds,
#  steps_per_epoch=100,
#  epochs=15,
#  validation_steps=50,
#  verbose=2
#)
#from keras.optimizers import Adam
#opt = Adam(lr=0.001)

"""CIFAR 10 & CIFAR 100 CODE END"""



"""################################  MAIN CODE END ################################

"""

# Commented out IPython magic to ensure Python compatibility.
# Importing dependency
# %tensorflow_version 2.x
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras import datasets
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
import numpy as np
import tensorflow as tf

# Import Data
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Build Model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32,32, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(10))

# Model Summary
model.summary()

# Model Compile
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Define the Gradient Fucntion
epoch_gradient = []
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# Define the Gradient Function
@tf.function
def get_gradient_func(model):
    with tf.GradientTape() as tape:
       logits = model(train_images, training=True)
       loss = loss_fn(train_labels, logits)
    grad = tape.gradient(loss, model.trainable_weights)
    model.optimizer.apply_gradients(zip(grad, model.trainable_variables))
    return grad

# Define the Required Callback Function
class GradientCalcCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    grad = get_gradient_func(model)
    epoch_gradient.append(grad)

epoch = 4

print(train_images.shape, train_labels.shape)

model.fit(train_images, train_labels, epochs=epoch, validation_data=(test_images, test_labels), callbacks=[GradientCalcCallback()])

# (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type
gradient = np.asarray(epoch_gradient)
print("Total number of epochs run:", epoch)

!pip install seqeval
!pip install tensorflow-addons
import pandas as pd
import numpy as np
import math as m
from google.colab import drive
drive.mount('/content/drive')
import tensorflow as tf
import tensorflow_addons as tfa
from tqdm import tqdm,trange
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.layers import Activation
from tensorflow.keras.initializers import RandomNormal
#from seqeval.metrics import accuracy_score,f1_score #classification_report,
#from tensorflow.keras.models import Sequential
#from tensorflow.keras.layers import Dense, Dropout, Flatten, LeakyReLU
#from tensorflow.keras.layers import Conv2D, MaxPooling2D

###############################################################################################################################################################################
batch_size = 32
num_classes = 10
epochs = 15
(x_train, y_train),(x_test, y_test) = cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
optimizer = Adam(learning_rate=0.001)#,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False)
weight_init = RandomNormal()
bat_per_epoch = m.floor(len(x_train) / batch_size)
#metr = tf.keras.metrics.Accuracy()
######################################################## CUSTOM FUNCTIONS #####################################################################################################
def create_model(activation_fn):
    cifar10_model=tf.keras.models.Sequential()
    cifar10_model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding="same", activation= activation_fn, input_shape=[32,32,3]))
    cifar10_model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding="same", activation= activation_fn))
    cifar10_model.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2,padding='valid'))
    cifar10_model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding="same", activation= activation_fn))
    cifar10_model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding="same", activation= activation_fn))
    cifar10_model.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2,padding='valid'))
    cifar10_model.add(tf.keras.layers.Flatten())
    cifar10_model.add(tf.keras.layers.Dropout(0.5,noise_shape=None,seed=None))
    cifar10_model.add(tf.keras.layers.Dense(units=128,activation= activation_fn))
    cifar10_model.add(tf.keras.layers.Dense(units=10,activation='softmax'))
    return cifar10_model

def step(epoch_,real_x, real_y):
    with tf.GradientTape() as tape:
        tf.watch(x)
        y = x**3
        # Make prediction
        pred_y = model(real_x) #.reshape((-1, 28, 28, 1))
        # Calculate loss
        model_loss = tf.keras.losses.sparse_categorical_crossentropy(real_y, pred_y)

    #print('Epoch:',epoch_)
    #print('Loss: ',model_loss)
    #metr = tf.keras.metrics.sparse_categorical_accuracy(real_y, pred_y)
    #assert metr.shape == (2,)
    #print("Accuracy: {}".format(metr.numpy()))
    #metr.reset_state()
    #metr.update_state(real_y,pred_y)
    #metr.result().numpy()
    #print("F1-Score: {}".format(f1_score([pred_y], [real_y])))
    # Calculate gradients
    model_gradients = tape.gradient(model_loss, model.trainable_variables)
    #print('Gradients:',model_gradients)
    # Update model
    optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))

def act_sine(x):
    '''Sine'''
    return tf.sin(x)

def GCU(x):
    '''Growing Cosine Unit'''
    return x * tf.cos(x)

def cos_2(x):
    '''Cos_2'''
    return tf.square(x) * tf.cos(x)

def act_sign_sin(x):
    '''Act_Sign_Sin'''
    return tf.sign(x)*tf.square(tf.sin(x))

#def swish(x):
#   '''Swish'''
#  return x * tf.sigmoid(x)

def new_act_1(x):
    '''New_Act_1'''
    return tf.math.pow(x + 1, 3)

pi = tf.constant(m.pi)

def act_signum(x):
    '''Signum'''
    return tf.sign(x)

def act_identity(x):
    '''Identity'''
    return tf.identity(x)

#def act_step(x):
#    '''Step'''
#    #tf.step(x)
#    if tf.greater_equal(x,0):
#      return 1
#    return 0

def act_SiLU(x):
    '''SiLU'''
    return (x/(1+tf.math.exp(-x)))

def act_LiSHT(x):
    '''LiSHT'''
    return x*tf.tanh(x)

def act_ReSech(x):
    '''ReSech'''
    return x/tf.tanh(x)

def act_bipolar(x):
    '''Bipolar'''
    return ((1-tf.math.exp(-x)) / (1+tf.math.exp(-x)))

def act_absolute(x):
    '''Absolute'''
    return abs(x)

def act_elliott(x):
    '''Elliott'''
    return (1/(1+abs(x)))

def act_quadratic(x):
    '''Quadratic'''
    return (tf.math.pow(x,2) + x)

def act_Mcubic(x):
    '''Monotonic Cubic'''
    return (tf.math.pow(x,3) + x)

def act_NMcubic(x):
    '''Non Monotonic Cubic'''
    return (x - tf.math.pow(x,3))

def act_DSU(x):
    '''Decaying Sine Unit'''
    return (pi/2)*(tf.experimental.numpy.sinc(x-pi) - tf.experimental.numpy.sinc(x+pi)) #((tf.sin(x - pi)/(x - pi)) - (tf.sin(x + pi)/(x + pi)))

#actual neuronal voltages, action potential
def act_shiftedSinc(x):
    '''Shifted Sinc'''
    return pi*tf.experimental.numpy.sinc(x-pi)

## FILL IN NEW ACTIVATION FUNCTIONS HERE (IF ANY)

#'ThresholdedReLU',new_act_1,softmax,exponential,act_elliott,act_ReSech

activation_inputs = ['sigmoid','softplus','softsign',
                     'relu','PReLU','gelu','elu',
                     'LeakyReLU','tanh','selu',
                     act_absolute,
                     GCU,
                     act_DSU,
                     act_NMcubic,
                     act_SiLU,
                     act_LiSHT,
                     act_Mcubic,
                     act_quadratic,
                     act_sine,
                     cos_2,
                     tf.keras.activations.swish,
                     tfa.activations.mish,
                     act_sign_sin,
                     act_signum,
                     act_identity,
                     act_bipolar,act_shiftedSinc]

for q in range(0,1):
  for k in activation_inputs:
      if type(k) == str:
          continue
      print(k(float(q)))
      print(Activation(k))

################################ Training loop ################################################################################################################################
for activation_input in ['relu']:#activation_inputs:
    model = create_model(activation_input)
    for _ in trange(epochs, desc="Epoch"): #for epoch in range(epochs):
        #print('=', end='')
        for i in range(bat_per_epoch):
            n = i*batch_size
            step(_,x_train[n:n+batch_size], y_train[n:n+batch_size])
            break
        break

# Calculate accuracy
#model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy']) # Compile just for evaluation
#print('\n', model.evaluate(x_test, y_test, verbose=0)[1])

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
!pip install keras-tuner
!pip install tensorflow-addons
# %load_ext tensorboard

# Clear any logs from previous runs
!rm -rf ./logs/

import pandas as pd
#import numpy as np
from google.colab import drive
drive.mount('/content/drive')
import math as m

# Clear any logs from previous runs
!rm -rf /content/drive/My\ Drive/Colab\ Notebooks/SurveyPaper/logs/

import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.python.ops.numpy_ops import np_config
np_config.enable_numpy_behavior()

#from tensorboard.plugins.hparams import api as hp
from tensorflow.keras.datasets import cifar10
#from kerastuner.engine.hyperparameters import HyperParameters as HP
import tensorflow.keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LeakyReLU
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorboard.plugins.hparams import api as hp
import os

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

batch_size = 64 #32
num_classes = 10
epochs = 25

#relu_valloss = []
#relu_valacc = []
data_list = []

#fashion_mnist = tf.keras.datasets.fashion_mnist
(x_train, y_train),(x_test, y_test) = cifar10.load_data()
#y_train = tf.keras.utils.to_categorical(y_train, num_classes)
#y_test = tf.keras.utils.to_categorical(y_test, num_classes)
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train, x_test = x_train.astype('float32'), x_test.astype('float32')

#tf.compat.v1.disable_eager_execution()
#HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))
#HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1,0.25,0.3]))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam'],)) #, 'sgd'
HP_L_RATE= hp.HParam('learning_rate', hp.Discrete([1e-4]))#,1e-5,1e-6])) #hp.RealInterval
#HP.Choice(values=[1e-2, 1e-3, 1e-4])
#HP.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))

METRIC_ACCURACY = 'accuracy'

LOG_FILE = '/content/drive/My Drive/Colab Notebooks/SurveyPaper/logs/hparam_tuning/'
with tf.summary.create_file_writer(LOG_FILE).as_default():
  hp.hparams_config(
    hparams=[HP_OPTIMIZER,HP_L_RATE], #HP_DROPOUT,,HP_NUM_UNITS,HP_L_RATE
    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],
  )

def train_test_model(hparams, activation_fn):
  #model = tf.keras.models.Sequential([
  #  tf.keras.layers.Flatten(),
  #  tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=activation_fn),
  #  tf.keras.layers.Dropout(hparams[HP_DROPOUT]),
  #  tf.keras.layers.Dense(10, activation=activation_fn),
  #])

  model = Sequential()
  model.add(Conv2D(32, (3, 3), padding='same',
                  input_shape=x_train.shape[1:]))
  model.add(Activation(activation_fn))
  #model.add(Conv2D(32, (3, 3)))
  #model.add(Activation(activation_fn))
  #model.add(MaxPooling2D(pool_size=(2, 2)))
  #model.add(Dropout(hparams[HP_DROPOUT]))

  #model.add(Conv2D(64, (3, 3), padding='same'))
  #model.add(Activation(activation_fn))
  #model.add(Conv2D(64, (3, 3)))
  #model.add(Activation(activation_fn))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  #model.add(Dropout(hparams[HP_DROPOUT]))

  model.add(Flatten())

  model.add(Dense(64))
  model.add(Activation(activation_fn)) #dense_activation
  model.add(Dropout(0.5))
  model.add(Dense(num_classes))
  model.add(Activation('softmax'))


  optimizer_name = hparams[HP_OPTIMIZER]
  learning_rate = hparams[HP_L_RATE]
  if optimizer_name == "adam":
      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
  elif optimizer_name == "sgd":
      optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
  else:
      raise ValueError("unexpected optimizer name: %r" % (optimizer_name,))

  model.compile(
      optimizer=optimizer,
      loss='sparse_categorical_crossentropy', #sparse_categorical_crossentropy
      metrics=['sparse_categorical_accuracy'], #accuracy
      )
  history1 = model.fit(x_train,
                       y_train,
                       batch_size=batch_size,
                       epochs=epochs,
                       #steps_per_epoch=781,
                       validation_split = 0.2,
                       shuffle=True)
  results = model.evaluate(x_test, y_test, batch_size=batch_size)

  act_func_str = activation_fn

  if act_func_str == tfa.activations.mish:
      act_func_str = 'Mish'

  if act_func_str == tf.keras.activations.swish:
      act_func_str = 'Swish'

  if type(act_func_str) != str:
      act_func_str = act_func_str.__doc__

  data_list.append([act_func_str,hparams[HP_L_RATE],results[0],results[1]]) #hparams[HP_DROPOUT]
  #relu_valloss.append(results[0])
  #relu_valacc.append(results[1])
  #model.fit(x_train, y_train, epochs=epochs,batch_size=batch_size) # Run with 1 epoch to speed things up for demo purposes
  #_, accuracy = model.evaluate(x_test, y_test)
  return results[1]

#from tensorflow.keras.utils import plot_model
#plot_model(relu_model, show_shapes=True, to_file='relu_model_architecture.png')
#def make_model(activation_fn):
#  model = Sequential()
#  model.add(Conv2D(32, (3, 3), padding='same',
#                  input_shape=x_train.shape[1:]))
#  model.add(Activation(activation_fn))
#  model.add(Conv2D(32, (3, 3)))
#  model.add(Activation(activation_fn))
#  model.add(MaxPooling2D(pool_size=(2, 2)))
#  #model.add(Dropout(hparams[HP_DROPOUT]))

#  model.add(Conv2D(64, (3, 3), padding='same'))
#  model.add(Activation(activation_fn))
#  model.add(Conv2D(64, (3, 3)))
#  model.add(Activation(activation_fn))
#  model.add(MaxPooling2D(pool_size=(2, 2)))
  #model.add(Dropout(hparams[HP_DROPOUT]))

#  model.add(Flatten())
#  model.add(Dense(64))
#  model.add(Activation(activation_fn)) #dense_activation
#  model.add(Dropout(0.5))
#  model.add(Dense(num_classes))
#  model.add(Activation('softmax'))
#  return model

#  plot_model(make_model(act_quadratic), show_shapes=True, to_file='/content/drive/My Drive/Colab Notebooks/SurveyPaper/model_architecture_1.png')

def run(run_dir, hparams,activation_input):
    with tf.summary.create_file_writer(run_dir).as_default():
        hp.hparams(hparams)  # record the values used in this trial
        accuracy = train_test_model(hparams,activation_input)
        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)

"""ADD ANY NEW ACTIVATION FUNCTIONS IN THE SNIPPET BELOW"""

#################### ALREADY DONE #########################

def act_sine(x):
    '''Sine'''
    return tf.sin(x)

def GCU(x):
    '''Growing Cosine Unit'''
    return x * tf.cos(x)

def cos_2(x):
    '''Cos_2'''
    return tf.square(x) * tf.cos(x)

def act_sign_sin(x):
    '''Act_Sign_Sin'''
    return tf.sign(x)*tf.square(tf.sin(x))

#def swish(x):
#   '''Swish'''
#  return x * tf.sigmoid(x)

def new_act_1(x):
    '''New_Act_1'''
    return tf.math.pow(x + 1, 3)

pi = tf.constant(m.pi)

def act_signum(x):
    '''Signum'''
    return tf.sign(x)

def act_identity(x):
    '''Identity'''
    return tf.identity(x)

#def act_step(x):
#    '''Step'''
#    #tf.step(x)
#    if tf.greater_equal(x,0):
#      return 1
#    return 0

def act_SiLU(x):
    '''SiLU'''
    return (x/(1+tf.math.exp(-x)))

def act_LiSHT(x):
    '''LiSHT'''
    return x*tf.tanh(x)

def act_ReSech(x):
    '''ReSech'''
    return x/tf.tanh(x)

def act_bipolar(x):
    '''Bipolar'''
    return ((1-tf.math.exp(-x)) / (1+tf.math.exp(-x)))

def act_absolute(x):
    '''Absolute'''
    return abs(x)

def act_elliott(x):
    '''Elliott'''
    return (1/(1+abs(x)))

def act_quadratic(x):
    '''Quadratic'''
    return (tf.math.pow(x,2) + x)

def act_Mcubic(x):
    '''Monotonic Cubic'''
    return (tf.math.pow(x,3) + x)

def act_NMcubic(x):
    '''Non Monotonic Cubic'''
    return (x - tf.math.pow(x,3))

def act_DSU(x):
    '''Decaying Sine Unit'''
    return (pi/2)*(tf.experimental.numpy.sinc(x-pi) - tf.experimental.numpy.sinc(x+pi)) #((tf.sin(x - pi)/(x - pi)) - (tf.sin(x + pi)/(x + pi)))

#actual neuronal voltages, action potential
def act_shiftedSinc(x):
    '''Shifted Sinc'''
    return pi*tf.experimental.numpy.sinc(x-pi)

#################### TO BE DONE #########################

## FILL IN NEW ACTIVATION FUNCTIONS HERE (IF ANY)

#'ThresholdedReLU',new_act_1,softmax,exponential,act_elliott,act_ReSech


#activation_inputs = ['sigmoid','softplus','softsign',
#                     'relu','PReLU','gelu','elu',
#                     'LeakyReLU','tanh','selu',
#                     act_absolute,
#                     GCU,
#                     act_DSU,
#                     act_NMcubic,
#                     act_SiLU,
#                     act_LiSHT,
#                     act_Mcubic,
#                     act_quadratic,
#                     act_sine,
#                     cos_2,
#                     tf.keras.activations.swish,
#                     tfa.activations.mish,
#                     act_sign_sin,
#                     act_signum,
#                     act_identity,
#                     act_bipolar]

# rechecking #
activation_inputs = [act_shiftedSinc]#,act_DSU,act_Mcubic]

for q in range(0,1):
  for k in activation_inputs:
      if type(k) == str:
          continue
      print(k(float(q)))
      print(Activation(k))

epochs = 20

data_list = []
session_num = 0
#for num_units in HP_NUM_UNITS.domain.values:
for activation_input in activation_inputs:
    #for dropout_rate in HP_DROPOUT.domain.values:#(HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
    for optimizer in HP_OPTIMIZER.domain.values:
        for lr in HP_L_RATE.domain.values:#(HP_L_RATE.domain.min_value,HP_L_RATE.domain.max_value):
            hparams = {
                #HP_NUM_UNITS: num_units,
                #HP_DROPOUT: dropout_rate,
                HP_OPTIMIZER: optimizer,
                HP_L_RATE: lr#{1e-2:1e-2,1e-3:1e-3,1e-4:1e-4}#tf.random.uniform((1,3),1e-2,1e-4) #HP_L_RATE
                }
            run_name = "run-{}-{}".format(activation_input,session_num)
            print('Activation Function:',activation_input,'--- Starting trial: %s' % run_name)
            print({h.name: hparams[h] for h in hparams})
            run(LOG_FILE + run_name, hparams,activation_input)
            session_num += 1

data = pd.DataFrame(data_list)
data = data.rename(columns={0:'Activation Function',1:'Learning Rate',2:'Loss',3:'Accuracy'})
data.to_csv('/content/drive/My Drive/Colab Notebooks/SurveyPaper/AFOutputs_NewRun_7.csv',index=False)

###################################################################################################################################################################################

session_num = 0
#for num_units in HP_NUM_UNITS.domain.values:
for activation_input in activation_inputs:
    for dropout_rate in HP_DROPOUT.domain.values:#(HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
        for optimizer in HP_OPTIMIZER.domain.values:
            for lr in HP_L_RATE.domain.values:#(HP_L_RATE.domain.min_value,HP_L_RATE.domain.max_value):
                hparams = {
                    #HP_NUM_UNITS: num_units,
                    HP_DROPOUT: dropout_rate,
                    HP_OPTIMIZER: optimizer,
                    HP_L_RATE: lr#{1e-2:1e-2,1e-3:1e-3,1e-4:1e-4}#tf.random.uniform((1,3),1e-2,1e-4) #HP_L_RATE
                    }
                run_name = "run-{}-{}".format(activation_input,session_num)
                print('Activation Function:',activation_input,'--- Starting trial: %s' % run_name)
                print({h.name: hparams[h] for h in hparams})
                run(LOG_FILE + run_name, hparams,activation_input)
                session_num += 1

data = pd.DataFrame(data_list)
data = data.rename(columns={0:'Activation Function',1:'Learning Rate',2:'Dropout',3:'Loss',4:'Accuracy'})
data.to_csv('/content/drive/My Drive/Colab Notebooks/SurveyPaper/AFOutputs_NewRun_6.csv',index=False)

###################################################################################################################################################################################

########## FIRST SET OF ACTIVATION FUNCTIONS #################################

session_num = 0
#for num_units in HP_NUM_UNITS.domain.values:

for activation_input in activation_inputs:
    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
        for optimizer in HP_OPTIMIZER.domain.values:
            for lr in (HP_L_RATE.domain.min_value,HP_L_RATE.domain.max_value):
                hparams = {
                    #HP_NUM_UNITS: num_units,
                    HP_DROPOUT: dropout_rate,
                    HP_OPTIMIZER: optimizer,
                    HP_L_RATE: lr#{1e-2:1e-2,1e-3:1e-3,1e-4:1e-4}#tf.random.uniform((1,3),1e-2,1e-4) #HP_L_RATE
                    }
                run_name = "run-{}-{}".format(activation_input,session_num)
                print('Activation Function:',activation_input,'--- Starting trial: %s' % run_name)
                print({h.name: hparams[h] for h in hparams})
                run(LOG_FILE + run_name, hparams,activation_input)
                session_num += 1

data = pd.DataFrame(data_list)
data = data.rename(columns={0:'Activation Function',1:'Learning Rate',2:'Dropout',3:'Loss',4:'Accuracy'})
data.to_csv('/content/drive/My Drive/Colab Notebooks/SurveyPaper/AFOutputs.csv',index=False)

#import shutil
#src='logs/hparam_tuning'
#dst='/content/drive/My Drive/Colab Notebooks/SurveyPaper'
#shutil.copytree(src, dst)















import kerastuner as kt

tuner = kt.Hyperband(
    build_model,
    objective='val_accuracy',
    max_epochs=30,
    hyperband_iterations=2)



import tensorflow as tf

def build_model(hp):


  model = tf.keras.Model(inputs, outputs)
  model.compile(
    optimizer=tf.keras.optimizers.Adam(
      hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])
  return model

import tensorflow_datasets as tfds

data = tfds.load('cifar10')
train_ds, test_ds = data['train'], data['test']

def standardize_record(record):
  return tf.cast(record['image'], tf.float32) / 255., record['label']

train_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)
test_ds = test_ds.map(standardize_record).cache().batch(64)

tuner.search(train_ds,
             validation_data=test_ds,
             epochs=30,
             callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])

best_model = tuner.get_best_models(1)[0]

best_hyperparameters = tuner.get_best_hyperparameters(1)[0]









def create_model(activation, dense_activation):
  model = Sequential()
  model.add(Conv2D(32, (3, 3), padding='same',
                  input_shape=x_train.shape[1:]))
  model.add(Activation(activation))
  model.add(Conv2D(32, (3, 3)))
  model.add(Activation(activation))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.25))

  model.add(Conv2D(64, (3, 3), padding='same'))
  model.add(Activation(activation))
  model.add(Conv2D(64, (3, 3)))
  model.add(Activation(activation))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.25))

  model.add(Flatten())
  model.add(Dense(512))
  model.add(Activation(dense_activation))
  model.add(Dropout(0.5))
  model.add(Dense(num_classes))
  model.add(Activation('softmax'))

  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])


  # initiate RMSprop optimizer
  opt = keras.optimizers.Adam(learning_rate=hp_learning_rate)
  #tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)

  # Let's train the model using RMSprop
  model.compile(loss='categorical_crossentropy',
                optimizer=opt,
                metrics=['accuracy'])

  return model

def act_sign_sin(x):
  return tf.sign(x)*tf.square(tf.sin(x))

def cos_1(x):
  return x * tf.cos(x)









import matplotlib.pyplot as plt

relu_valloss = []
relu_valacc = []
for i in range(0, 1):
  print("Epoch: {}".format(i))
  relu_model = create_model('relu', 'relu')
  history1 = relu_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = relu_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  relu_valloss.append(results[0])
  relu_valacc.append(results[1])

!git clone https://github.com/artur-deluca/landscapeviz.git

import landscapeviz

landscapeviz.build_mesh(relu_model, (x_train, y_train), grid_length=40, verbose=0)
landscapeviz.plot_contour(key="categorical_crossentropy")
landscapeviz.plot_3d(key="categorical_crossentropy")



relu_model.save('relu_model.h5')

import matplotlib.pyplot as plt
import numpy as np

len(history1.history['loss'])
num_epochs = range(0, 25)

num_epochs

plt.plot(num_epochs, history1.history['loss'])
plt.legend(['loss-relu'])
plt.savefig('relu-loss.png')

plt.plot(num_epochs, history1.history['accuracy'])
plt.legend(['accuracy-relu'])
plt.savefig('relu-acc.png')

plt.plot(num_epochs, history1.history['val_loss'])
plt.legend(['val_loss-relu'])
plt.savefig('relu-valloss.png')

plt.plot(num_epochs, history1.history['val_accuracy'])
plt.legend(['val_acc-relu'])
plt.savefig('relu-val_acc.png')

from tensorflow.keras.utils import plot_model
#plot_model(relu_model, show_shapes=True, to_file='relu_model_architecture.png')

plot_model(relu_model, show_shapes=True, to_file='relu_model_architecture.png')

sum(relu_valacc) / len(relu_valacc)

xcosx_valloss = []
xcos_valacc = []
for i in range(0, 5):
  print("Epoch: {}".format(i))
  cos1_model = create_model(cos_1, cos_1)
  history2 = cos1_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = cos1_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  xcosx_valloss.append(results[0])
  xcos_valacc.append(results[1])



xcosx_valloss = []
xcos_valacc = []
for i in range(0, 1):
  print("Epoch: {}".format(i))
  cos1_model = create_model(cos_1, 'relu')
  history2 = cos1_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = cos1_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  xcosx_valloss.append(results[0])
  xcos_valacc.append(results[1])

plt.plot(num_epochs, history2.history['loss'])
plt.legend(['loss-xcosx'])
plt.savefig('xcosx-loss.png')

plt.plot(num_epochs, history2.history['accuracy'])
plt.legend(['accuracy-xcosx'])
plt.savefig('xcosx-acc.png')

plt.plot(num_epochs, history2.history['val_loss'])
plt.legend(['val_loss-xcosx'])
plt.savefig('xcosx-valloss.png')

plt.plot(num_epochs, history2.history['val_accuracy'])
plt.legend(['val_acc-xcosx'])
plt.savefig('xcosx-val_acc.png')

sum(xcos_valacc) / len(xcos_valacc)

swish_valloss = []
swish_valacc = []
for i in range(0, 1):
  swish_model = create_model(tf.keras.activations.swish, tf.keras.activations.swish)
  history3 = swish_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = swish_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  swish_valloss.append(results[0])
  swish_valacc.append(results[1])

plt.plot(num_epochs, history3.history['loss'])
plt.legend(['loss-all_swish'])
plt.savefig('all_swish-loss.png')

plt.plot(num_epochs, history3.history['accuracy'])
plt.legend(['accuracy-all_swish'])
plt.savefig('all_swish-acc.png')

plt.plot(num_epochs, history3.history['val_loss'])
plt.legend(['val_loss-all_swish'])
plt.savefig('all_swish-valloss.png')

plt.plot(num_epochs, history3.history['val_accuracy'])
plt.legend(['val_acc-all_swish'])
plt.savefig('all_swish-val_acc.png')

sum(swish_valacc) / len(swish_valacc)

!pip install tensorflow-addons



mish_valloss = []
mish_valacc = []
for i in range(0, 1):
  mish_model = create_model(tfa.activations.mish, tfa.activations.mish)
  history4 = mish_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = mish_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  mish_valloss.append(results[0])
  mish_valacc.append(results[1])

plt.plot(num_epochs, history8.history['loss'])
plt.legend(['loss-all_mish'])
plt.savefig('all_mish-loss.png')

plt.plot(num_epochs, history8.history['accuracy'])
plt.legend(['accuracy-all_mish'])
plt.savefig('all_mish-acc.png')

plt.plot(num_epochs, history8.history['val_loss'])
plt.legend(['val_loss-all_mish'])
plt.savefig('all_mish-valloss.png')

plt.plot(num_epochs, history8.history['val_accuracy'])
plt.legend(['val_acc-all_mish'])
plt.savefig('all_mish-val_acc.png')



print(sum(mish_valacc) / len(mish_valacc))

hybrid_mish_valloss = []
hybrid_mish_valacc = []
for i in range(0, 1):
  print("Epoch: {}".format(i))
  mish_model = create_model(tfa.activations.mish, 'relu')
  history5 = mish_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = mish_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  hybrid_mish_valloss.append(results[0])
  hybrid_mish_valacc.append(results[1])

plt.plot(num_epochs, history4.history['loss'])
plt.legend(['loss-hybrid_mish'])
plt.savefig('hybrid_mish-loss.png')

plt.plot(num_epochs, history4.history['accuracy'])
plt.legend(['accuracy-hybrid_mish'])
plt.savefig('hybrid_mish-acc.png')

plt.plot(num_epochs, history4.history['val_loss'])
plt.legend(['val_loss-hybrid_mish'])
plt.savefig('hybrid_mish-valloss.png')

plt.plot(num_epochs, history4.history['val_accuracy'])
plt.legend(['val_acc-hybrid_mish'])
plt.savefig('hybrid_mish-val_acc.png')



hybrid_swish_valloss = []
hybrid_swish_valacc = []
for i in range(0, 1):
  print("Epoch {}".format(i))
  swish_model = create_model(tf.keras.activations.swish, 'relu')
  history6 = swish_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = swish_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  hybrid_swish_valloss.append(results[0])
  hybrid_swish_valacc.append(results[1])

tanh_all_valloss = []
tanh_all_valacc = []
for i in range(0, 5):
  print("Epoch: {}".format(i))
  tanh_model = create_model('tanh', 'tanh')
  history7 = tanh_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = tanh_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  tanh_all_valloss.append(results[0])
  tanh_all_valacc.append(results[1])

tanh_hybrid_valloss = []
tanh_hybrid_valacc = []
for i in range(0, 5):
  print("Epoch: {}".format(i))
  tanh_hybrid_model = create_model('tanh', 'relu')
  history8 = tanh_hybrid_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = tanh_hybrid_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  tanh_hybrid_valloss.append(results[0])
  tanh_hybrid_valacc.append(results[1])

sigmoid_hybrid_valloss = []
sigmoid_hybrid_valacc = []
for i in range(0, 5):
  print("Epoch: {}".format(i))
  sigmoid_hybrid_model = create_model('sigmoid', 'relu')
  history9 = sigmoid_hybrid_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = sigmoid_hybrid_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  sigmoid_hybrid_valloss.append(results[0])
  sigmoid_hybrid_valacc.append(results[1])

sigmoid_all_valloss = []
sigmoid_all_valacc = []
for i in range(0, 5):
  print("Epoch: {}".format(i))
  sigmoid_model = create_model('sigmoid', 'sigmoid')
  history10 = sigmoid_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True)
  # y_pred = cos1_model.predict(x_test)
  results = sigmoid_model.evaluate(x_test, y_test, batch_size=batch_size)
  print(results)
  # acc = accuracy_score(y_test, y_pred)
  sigmoid_all_valloss.append(results[0])
  sigmoid_all_valacc.append(results[1])



import matplotlib.pyplot as plt

plt.plot(num_epochs, history1.history['loss'], label = 'relu')
plt.plot(num_epochs, history2.history['loss'], label = 'zcosz-hybrid')
plt.plot(num_epochs, history3.history['loss'], label = 'swish')
plt.plot(num_epochs, history4.history['loss'], label = 'mish')
plt.plot(num_epochs, history5.history['loss'], label = 'mish-hybrid')
plt.plot(num_epochs, history6.history['loss'], label = 'swish-hybrid')
plt.legend()
plt.savefig('loss.png')

plt.plot(num_epochs, history1.history['accuracy'], label = 'relu')
plt.plot(num_epochs, history2.history['accuracy'], label = 'zcosz-hybrid')
plt.plot(num_epochs, history3.history['accuracy'], label = 'swish')
plt.plot(num_epochs, history4.history['accuracy'], label = 'mish')
plt.plot(num_epochs, history5.history['accuracy'], label = 'mish-hybrid')
plt.plot(num_epochs, history6.history['accuracy'], label = 'swish-hybrid')
plt.legend()
plt.savefig('accuracy.png')

plt.plot(num_epochs, history1.history['val_loss'], label = 'relu')
plt.plot(num_epochs, history2.history['val_loss'], label = 'zcosz-hybrid')
plt.plot(num_epochs, history3.history['val_loss'], label = 'swish')
plt.plot(num_epochs, history4.history['val_loss'], label = 'mish')
plt.plot(num_epochs, history5.history['val_loss'], label = 'mish-hybrid')
plt.plot(num_epochs, history6.history['val_loss'], label = 'swish-hybrid')
plt.legend()
plt.savefig('val_loss.png')

plt.plot(num_epochs, history1.history['val_accuracy'], label = 'relu')
plt.plot(num_epochs, history2.history['val_accuracy'], label = 'zcosz-hybrid')
plt.plot(num_epochs, history3.history['val_accuracy'], label = 'swish')
plt.plot(num_epochs, history4.history['val_accuracy'], label = 'mish')
plt.plot(num_epochs, history5.history['val_accuracy'], label = 'mish-hybrid')
plt.plot(num_epochs, history6.history['val_accuracy'], label = 'swish-hybrid')
plt.legend()
plt.savefig('val_accuracy.png')

plt.plot(num_epochs, history5.history['loss'])
plt.legend(['loss-hybrid_swish'])
plt.savefig('hybrid_swish-loss.png')

plt.plot(num_epochs, history5.history['accuracy'])
plt.legend(['accuracy-hybrid_swish'])
plt.savefig('hybrid_swish-acc.png')

plt.plot(num_epochs, history5.history['val_loss'])
plt.legend(['val_loss-hybrid_swish'])
plt.savefig('hybrid_swish-valloss.png')

plt.plot(num_epochs, history5.history['val_accuracy'])
plt.legend(['val_acc-hybrid_swish'])
plt.savefig('hybrid_swish-val_acc.png')



print(sum(hybrid_swish_valacc) / len(hybrid_swish_valacc))

hybrid_mish_valacc

results_valacc_dict = {
    "xcosx-all": xcos_valacc,
    "tanh-all": tanh_all_valacc,
    "tanh-hybrid": tanh_hybrid_valacc,
    "sigmoid-all": sigmoid_all_valacc,
    "sigmoid-hybrid": sigmoid_hybrid_valacc
}

results_valloss_dict = {
    "xcosx-all": xcosx_valloss,
    "tanh-all": tanh_all_valloss,
    "tanh-hybrid": tanh_hybrid_valloss,
    "sigmoid-all": sigmoid_all_valloss,
    "sigmoid-hybrid": sigmoid_hybrid_valloss
}

with open('results_loss.csv', 'w') as f:
    for key in results_valloss_dict.keys():
        f.write("%s,%s\n"%(key,results_valloss_dict[key]))

with open('results_acc.csv', 'w') as f:
    for key in results_valacc_dict.keys():
        f.write("%s,%s\n"%(key,results_valacc_dict[key]))

print("Testing Accuracy for swish-all: {}".format(all_swish_valacc/10))
print("Testing Accuracy for mish-all: {}".format(all_mish_valacc/10))

print("Testing Accuracy for swish-hybrid: {}".format(swish_valacc/10))
print("Testing Accuracy for mish-hybrid: {}".format(mish_valacc/10))

print("Testing Accuracy for relu: {}".format(relu_valacc/10))
print("Testing Accuracy for x_cosx: {}".format(xcos_valacc/10))

sin_model = create_model(tf.sin)
relu_model = create_model('relu')
sign_sin_model = create_model(act_sign_sin)
cos_model = create_model(tf.cos)
cos1_model = create_model(cos_1)
cos2_model = create_model(cos_2)



"""## Other functions"""

callback = Tensordash(
    ModelName = "Sine Model",
    email = email,
    password = password)

try:

  history1 = sin_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_split = 0.2,
            shuffle=True,
            callbacks = [callback])
except:
  callback.sendCrash()

callback = Tensordash(
    ModelName = "ReLU Model",
    email = email,
    password = password)

try:

  history2 = relu_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(x_test, y_test),
            shuffle=True,
            callbacks = [callback])
except:
  callback.sendCrash()

callback = Tensordash(
    ModelName = "Sign Sine Model",
    email = email,
    password = password)

try:

  history3 = sign_sin_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(x_test, y_test),
            shuffle=True,
            callbacks = [callback])
except:
  callback.sendCrash()

callback = Tensordash(
    ModelName = "Cosine Model",
    email = email,
    password = password)

try:

  history4 = cos_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(x_test, y_test),
            shuffle=True,
            callbacks = [callback])
except:
  callback.sendCrash()

from sklearn.metrics import accuracy_score

callback = Tensordash(
    ModelName = "Cosine 2 Model",
    email = email,
    password = password)

try:

  history6 = cos2_model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(x_test, y_test),
            shuffle=True,
            callbacks = [callback])
except:
  callback.sendCrash()

model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',
                input_shape=x_train.shape[1:]))
model.add(LeakyReLU(0.2))
model.add(Conv2D(32, (3, 3)))
model.add(LeakyReLU(0.2))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding='same'))
model.add(LeakyReLU(0.2))
model.add(Conv2D(64, (3, 3)))
model.add(LeakyReLU(0.2))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))

# initiate RMSprop optimizer
opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)

# Let's train the model using RMSprop
model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

callback = Tensordash(
    ModelName = "Leaky ReLU Model",
    email = email,
    password = password)

try:

  history7 = model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(x_test, y_test),
            shuffle=True,
            callbacks = [callback])
except:
  callback.sendCrash()

!git clone https://github.com/tomgoldstein/loss-landscape

cp relu_model.h5 loss-landscape/

cd loss-landscape/

!python plot_2D.py --surf_file relu_model.h5 --surf_name train_loss

import h5py

f = h5py.File('relu_model.h5','r')

f

f['xcoordinates']

